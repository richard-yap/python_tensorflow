{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Edited1 3-diabetic-NeuralNet-Exercise-TF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoTiGFdTGgja",
        "colab_type": "text"
      },
      "source": [
        "### Predict Diabetes from Medical Records::\n",
        "#### Dataset (Pima Indians Database): (https://www.kaggle.com/uciml/pima-indians-diabetes-database)\n",
        " - Features: Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, and Age\n",
        " - Outcome: Normal(0) vs Diabetes(1)\n",
        " \n",
        "### Simple Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrthSu_HGp2B",
        "colab_type": "code",
        "outputId": "a6c77b29-af01-484f-8c54-bfc3e0cf030d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRJ462toG4j6",
        "colab_type": "code",
        "outputId": "7a3b3cd1-9d24-4e55-d951-e171d8959e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB02x-4jG4tV",
        "colab_type": "code",
        "outputId": "e360e270-d735-455e-c6ff-c70cf2c77225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd drive/My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce45fJg0G4zf",
        "colab_type": "code",
        "outputId": "1b1448bf-91a3-460f-e154-ea9c50c01d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Business Model Canvas.gdoc'  'Leftovers .gdoc'\n",
            " \u001b[0m\u001b[01;34mCITREP_Data+Code\u001b[0m/            'Oral Presentation.gdoc'\n",
            " \u001b[01;34mClassroom\u001b[0m/                   'Quality Control.gdoc'\n",
            "\u001b[01;34m'Colab Notebooks'\u001b[0m/            'Quality Control Slides.gslides'\n",
            "'Getting started.pdf'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylWPNDfdG8Dh",
        "colab_type": "code",
        "outputId": "f9da42bb-69d6-460a-b81f-9f3907db894c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd CITREP_Data+Code/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CITREP_Data+Code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqmJrG1rG8gy",
        "colab_type": "code",
        "outputId": "d4c0b4fb-8431-47a6-8911-51523e67086c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 2a-TensorFlow-Data-Loading.ipynb\n",
            " 2b-TensorFlow-ML.ipynb\n",
            " 3a-TF-FcNN-MNIST.ipynb\n",
            " 3b-TF-FcNN-MNIST-Exercise.ipynb\n",
            " 3c-FcNN-CIFAR10.ipynb\n",
            " 3d-FcNN-CIFAR10-Exercise.ipynb\n",
            " 3-diabetic-NeuralNet-Exercise-TF.ipynb\n",
            " 3e-keras-FCNN-MNIST.ipynb\n",
            " 3f-keras-FcNN-dogscats.ipynb\n",
            " 3g-keras-FcNN-bloodcell-Exercise.ipynb\n",
            " 3h-keras-FC-AutoEncoder.ipynb\n",
            " 4a-MNIST-CNN-TF.ipynb\n",
            " 4b-CIFAR10-CNN-TF-Exercise.ipynb\n",
            " 5a-keras-CNN-dogscats.ipynb\n",
            " 5b-keras-CNN-Bloodcell.ipynb\n",
            " 5c-keras-Con-AutoEncoder.ipynb\n",
            " 5d-keras-Vgg16-dogscats.ipynb\n",
            " 5e-Resnet50-keras-dogscats-Exercise.ipynb\n",
            " 6-Semeion-Classification-SimpleCNN-Exercise.ipynb\n",
            " 7-RNN-IMDB.ipynb\n",
            "'CITREP+ - Clarence & Dr Sudipta - Deep Learning with Tensorflow and Python - v2.pdf'\n",
            " \u001b[0m\u001b[01;34mData\u001b[0m/\n",
            " \u001b[01;34mmnist\u001b[0m/\n",
            " \u001b[01;34mmodels\u001b[0m/\n",
            " tensorboard.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdnHJnZUGgjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import Imputer\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAAohzR3Ggje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build model with learning rate = 0.01\n",
        "# Number od epochs = 10\n",
        "learning_rate = 0.01\n",
        "training_epochs = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzV8Rho5Ggjg",
        "colab_type": "code",
        "outputId": "ebb04380-ef3e-49e5-b99c-f0198b7043ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "#if you don't want to transfer the file out to the same folder as the notebook, just do Data/...\n",
        "df = pd.read_csv(\"Data/diabetes.csv\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl67JElHGgji",
        "colab_type": "text"
      },
      "source": [
        "### Checking Missing/Null Values for different Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byACf7ZCGgjj",
        "colab_type": "code",
        "outputId": "bebbafe3-20c9-41a6-a157-e94618ae12ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "df1 = df.iloc[:, :-1]\n",
        "print(\"\\nColumn Name           % of Null Values\\n\")\n",
        "# print((df1[:] == 0).sum())\n",
        "((df1[:] == 0).sum())/768*100"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Column Name           % of Null Values\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pregnancies                 14.453125\n",
              "Glucose                      0.651042\n",
              "BloodPressure                4.557292\n",
              "SkinThickness               29.557292\n",
              "Insulin                     48.697917\n",
              "BMI                          1.432292\n",
              "DiabetesPedigreeFunction     0.000000\n",
              "Age                          0.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCFY5u_RGgjl",
        "colab_type": "code",
        "outputId": "6c657a1a-6b1c-4c10-b790-a14760b80ca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "source": [
        "# this plots the correlation data between each of the parameters\n",
        "corr = df[df.columns].corr()\n",
        "sns.heatmap(corr, cmap=\"Greens\", annot = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f85f9c37c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdcAAAF2CAYAAAA8xNKKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAM3EFQCabZrXHmv68ZdTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBovQ6ADGgjn",
        "colab_type": "text"
      },
      "source": [
        "#### From the correlation plot, it is clear that Features with null values (Insulin and SkinThickness) are not correlated with the outcome.\n",
        "### Prepare Dataset for Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w830G7ZGgjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# google the iloc() function\n",
        "X = df.iloc[:, :-1] #everything in the dataset df except for the last column \"outcome\"\n",
        "y = df.iloc[:, -1] # y contains only the \"outcome\" column\n",
        "X_trainold, X_testold, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y-sSo44NJrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df.iloc[:, :-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyu1iZMxGgjq",
        "colab_type": "code",
        "outputId": "5db7531a-faa9-426e-8fcb-bc9a99d33dd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"X_shape:\",X.shape,\"Y_shape:\",y.shape)\n",
        "print(\"X_Train:\",X_trainold.shape,\"X_Test:\", X_testold.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_shape: (768, 8) Y_shape: (768,)\n",
            "X_Train: (614, 8) X_Test: (154, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6soOGq4Ggjs",
        "colab_type": "text"
      },
      "source": [
        "### Missing Values are replaced by Median"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWY8vls1Ggjt",
        "colab_type": "code",
        "outputId": "04670e10-0875-4350-e0e7-c93d15beac8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "imputer = Imputer(missing_values=0,strategy='median')\n",
        "X_train = imputer.fit_transform(X_trainold)\n",
        "X_test = imputer.transform(X_testold)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUEMsEkdGgjv",
        "colab_type": "code",
        "outputId": "0a84d5d2-6b45-47f0-e6fd-19cf9c786553",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"X_Train:\",X_train.shape,\"X_Test:\", X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_Train: (614, 8) X_Test: (154, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcv_AgAzGgjy",
        "colab_type": "code",
        "outputId": "5f194473-33cf-4f1e-d5ec-d23419f5ae8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "mean = X_train.mean(axis=0)\n",
        "std = X_train.std(axis=0)\n",
        "print(\"Mean: \",mean)\n",
        "print(\"Std: \",std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean:  [  4.43973941 121.82899023  72.26221498  28.84201954 141.33224756\n",
            "  32.19723127   0.46379479  33.2980456 ]\n",
            "Std:  [ 2.99817272 30.47429215 12.40618104  8.5871349  87.34394387  6.81501567\n",
            "  0.327095   11.80062782]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg1rR-EBGgj0",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the data\n",
        "#### To deal with different scale for different features, need to do feature-wise normalization: For each feature substract the feature-mean and divide by the feature-standard_deviation. So that the feature is centered around 0 and has a unit standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDWzVPTOGgj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train -= mean\n",
        "X_train /= std\n",
        "\n",
        "X_test -= mean \n",
        "X_test /= std   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUUxlpN8Ggj3",
        "colab_type": "code",
        "outputId": "4034d51e-2f75-4c25-9804-a01eae463f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "X_train.mean(axis=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7.81134115e-17, 9.40254027e-17, 4.77359737e-16, 1.44654466e-16,\n",
              "       1.15723573e-17, 6.84938895e-16, 1.51887189e-16, 2.19874788e-16])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8IzMXpXGgj5",
        "colab_type": "code",
        "outputId": "9e6099d1-4554-4dc2-ccb4-e6a8893ee747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.std(axis=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upvahhOYGgj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preparing our levels as one hot encoding\n",
        "from keras.utils import to_categorical\n",
        "y_train1 = to_categorical(y_train)\n",
        "y_test1 = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NBMAMroGgj9",
        "colab_type": "code",
        "outputId": "26d77294-4dee-4512-a4c8-9fb86134f55a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train1[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPB-ccNLGgj_",
        "colab_type": "text"
      },
      "source": [
        "### Build the Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJHE5osiGgkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a FcNN model with 2 hidden layers (20 and 10 nurons respectively)\n",
        "# use sigmoid as activation function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUF98gUtOt1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------------------------------------------MY ANSWER----------------------------------------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsY7NC4AI3wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Hyper Parameters\n",
        "learning_rate = 0.01\n",
        "training_epochs = 500\n",
        "\n",
        "#data boxes (boxes contain 100 random data points from the dataset) \n",
        "#based on how much ram your com has, you can increase the batch size\n",
        "batch_size = 100 \n",
        "\n",
        "tf.set_random_seed(25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuX725VeJAhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "X = tf.placeholder(tf.float32, [None, 768*8])\n",
        "y = tf.placeholder(tf.float32, [None, 2]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rLDWgZOJs6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L1 = 20\n",
        "L2 = 10\n",
        "\n",
        "#in between the input layer \n",
        "W1 = tf.Variable(tf.truncated_normal([768*8, L1], stddev=0.1)) # 784 x 200 = 156800\n",
        "B1 = tf.Variable(tf.truncated_normal([L1],stddev=0.1)) # b1 = 200 because there are 200 neurons in hidden layer 1\n",
        "\n",
        "#in between hidden layer 1 and hideen layer 2\n",
        "W2 = tf.Variable(tf.truncated_normal([L1, L2], stddev=0.1))\n",
        "B2 = tf.Variable(tf.truncated_normal([L2],stddev=0.1))\n",
        "\n",
        "#in between hidden layer 2 and the output layer\n",
        "W3 = tf.Variable(tf.truncated_normal([L2, 2], stddev=0.1))\n",
        "B3 = tf.Variable(tf.truncated_normal([2],stddev=0.1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLeJrGoNKoKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y1 = tf.nn.sigmoid(tf.matmul(X, W1) + B1)\n",
        "Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + B2)\n",
        "\n",
        "Ylogits = tf.matmul(Y2, W3) + B3\n",
        "yhat = tf.nn.softmax(Ylogits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWdRA9U3GgkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.reduce_mean(\n",
        "   tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=Ylogits))\n",
        "\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNmgZHvSGgkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_correct = tf.equal(tf.argmax(y,1),tf.argmax(yhat,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y4hYNeEK_lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 5: Training Loop\n",
        "for epoch in range(training_epochs):\n",
        "  train_data = {X: batch_X, y: batch_y}\n",
        "  sess.run(train, feed_dict=train_data)\n",
        "  print(epoch*num_batches+i+1, \"Training accuracy =\", sess.run(accuracy, feed_dict=train_data),\n",
        "        \"Loss =\", sess.run(loss, feed_dict=train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr-hdp_WGgkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the performance with Test data\n",
        "test_data = {X:mnist.test.images,y:mnist.test.labels}\n",
        "print(\"Testing Accuracy = \", sess.run(accuracy, feed_dict = test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHFHPovROFb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#---------------------------------------------------------------------------------------TEACHER'S ANSWERS---------------------------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KP4vklPOKU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper Parameters\n",
        "learning_rate = 0.01\n",
        "training_epochs = 500\n",
        "\n",
        "tf.set_random_seed(50)\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 8]) \n",
        "y = tf.placeholder(tf.float32, [None, 2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jgd2UiROOjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L1 = 20\n",
        "L2 = 10\n",
        "\n",
        "#in between the input layer \n",
        "W1 = tf.Variable(tf.truncated_normal([8, L1], stddev=0.1)) \n",
        "B1 = tf.Variable(tf.truncated_normal([L1],stddev=0.1))\n",
        "\n",
        "#in between hidden layer 1 and hideen layer 2\n",
        "W2 = tf.Variable(tf.truncated_normal([L1, L2], stddev=0.1))\n",
        "B2 = tf.Variable(tf.truncated_normal([L2],stddev=0.1))\n",
        "\n",
        "#in between hidden layer 2 and the output layer\n",
        "W3 = tf.Variable(tf.truncated_normal([L2, 2], stddev=0.1))\n",
        "B3 = tf.Variable(tf.truncated_normal([2],stddev=0.1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JROlzUYGOX1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y1 = tf.nn.sigmoid(tf.matmul(X, W1) + B1)\n",
        "Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + B2)\n",
        "\n",
        "Ylogits = tf.matmul(Y2, W3) + B3\n",
        "yhat = tf.nn.softmax(Ylogits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bFosr67Oeqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.reduce_mean(\n",
        "   tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=Ylogits))\n",
        "\n",
        "train = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAzqA1LXOhlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_correct = tf.equal(tf.argmax(y,1),tf.argmax(yhat,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krSQ8n5LOioo",
        "colab_type": "code",
        "outputId": "7135d969-c9c3-4a19-a3c4-22b8c9aacf09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Step 5: Training Loop\n",
        "for epoch in range(training_epochs):\n",
        "  train_data = {X: X_train, y: y_train1}\n",
        "  sess.run(train, feed_dict=train_data)\n",
        "  print(epoch, \"Training accuracy =\", sess.run(accuracy, feed_dict=train_data),\n",
        "        \"Loss =\", sess.run(loss, feed_dict=train_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Training accuracy = 0.6530945 Loss = 0.6495516\n",
            "1 Training accuracy = 0.6530945 Loss = 0.6448405\n",
            "2 Training accuracy = 0.6530945 Loss = 0.64463186\n",
            "3 Training accuracy = 0.6530945 Loss = 0.64607024\n",
            "4 Training accuracy = 0.6530945 Loss = 0.6463362\n",
            "5 Training accuracy = 0.6530945 Loss = 0.6448942\n",
            "6 Training accuracy = 0.6530945 Loss = 0.64255667\n",
            "7 Training accuracy = 0.6530945 Loss = 0.64023095\n",
            "8 Training accuracy = 0.6530945 Loss = 0.6384355\n",
            "9 Training accuracy = 0.6530945 Loss = 0.63721484\n",
            "10 Training accuracy = 0.6530945 Loss = 0.6362623\n",
            "11 Training accuracy = 0.6530945 Loss = 0.6351488\n",
            "12 Training accuracy = 0.6530945 Loss = 0.63354\n",
            "13 Training accuracy = 0.6530945 Loss = 0.6313098\n",
            "14 Training accuracy = 0.6530945 Loss = 0.62852764\n",
            "15 Training accuracy = 0.6530945 Loss = 0.6253718\n",
            "16 Training accuracy = 0.6530945 Loss = 0.62202835\n",
            "17 Training accuracy = 0.6530945 Loss = 0.6186121\n",
            "18 Training accuracy = 0.6530945 Loss = 0.61513126\n",
            "19 Training accuracy = 0.6530945 Loss = 0.6115012\n",
            "20 Training accuracy = 0.6530945 Loss = 0.6075982\n",
            "21 Training accuracy = 0.6530945 Loss = 0.6033243\n",
            "22 Training accuracy = 0.6530945 Loss = 0.5986556\n",
            "23 Training accuracy = 0.6530945 Loss = 0.5936467\n",
            "24 Training accuracy = 0.6530945 Loss = 0.58840114\n",
            "25 Training accuracy = 0.6530945 Loss = 0.5830208\n",
            "26 Training accuracy = 0.6514658 Loss = 0.5775682\n",
            "27 Training accuracy = 0.65798044 Loss = 0.57205456\n",
            "28 Training accuracy = 0.67589575 Loss = 0.56646097\n",
            "29 Training accuracy = 0.6921824 Loss = 0.56077284\n",
            "30 Training accuracy = 0.70358306 Loss = 0.55500793\n",
            "31 Training accuracy = 0.71986973 Loss = 0.54922354\n",
            "32 Training accuracy = 0.7247557 Loss = 0.543503\n",
            "33 Training accuracy = 0.7296417 Loss = 0.537933\n",
            "34 Training accuracy = 0.73615634 Loss = 0.53258586\n",
            "35 Training accuracy = 0.7410423 Loss = 0.5275107\n",
            "36 Training accuracy = 0.74592835 Loss = 0.5227381\n",
            "37 Training accuracy = 0.74429965 Loss = 0.51828897\n",
            "38 Training accuracy = 0.7312704 Loss = 0.51418245\n",
            "39 Training accuracy = 0.732899 Loss = 0.5104343\n",
            "40 Training accuracy = 0.73615634 Loss = 0.50705045\n",
            "41 Training accuracy = 0.7394137 Loss = 0.5040174\n",
            "42 Training accuracy = 0.7394137 Loss = 0.5012978\n",
            "43 Training accuracy = 0.7345277 Loss = 0.49883595\n",
            "44 Training accuracy = 0.7345277 Loss = 0.49657103\n",
            "45 Training accuracy = 0.74592835 Loss = 0.49445036\n",
            "46 Training accuracy = 0.747557 Loss = 0.49243715\n",
            "47 Training accuracy = 0.7491857 Loss = 0.49050814\n",
            "48 Training accuracy = 0.7589577 Loss = 0.48864487\n",
            "49 Training accuracy = 0.7605863 Loss = 0.48682892\n",
            "50 Training accuracy = 0.767101 Loss = 0.4850434\n",
            "51 Training accuracy = 0.7687296 Loss = 0.48328084\n",
            "52 Training accuracy = 0.767101 Loss = 0.4815504\n",
            "53 Training accuracy = 0.76384366 Loss = 0.47987685\n",
            "54 Training accuracy = 0.76221496 Loss = 0.47829255\n",
            "55 Training accuracy = 0.7654723 Loss = 0.47682557\n",
            "56 Training accuracy = 0.767101 Loss = 0.4754926\n",
            "57 Training accuracy = 0.767101 Loss = 0.47429913\n",
            "58 Training accuracy = 0.7703583 Loss = 0.47324553\n",
            "59 Training accuracy = 0.77361566 Loss = 0.47232947\n",
            "60 Training accuracy = 0.77361566 Loss = 0.4715432\n",
            "61 Training accuracy = 0.77850163 Loss = 0.47086784\n",
            "62 Training accuracy = 0.776873 Loss = 0.47027633\n",
            "63 Training accuracy = 0.776873 Loss = 0.46974242\n",
            "64 Training accuracy = 0.77850163 Loss = 0.46925002\n",
            "65 Training accuracy = 0.77850163 Loss = 0.46879154\n",
            "66 Training accuracy = 0.77850163 Loss = 0.46836162\n",
            "67 Training accuracy = 0.776873 Loss = 0.46795347\n",
            "68 Training accuracy = 0.77361566 Loss = 0.46756095\n",
            "69 Training accuracy = 0.7687296 Loss = 0.4671832\n",
            "70 Training accuracy = 0.7687296 Loss = 0.46682233\n",
            "71 Training accuracy = 0.77198696 Loss = 0.46647793\n",
            "72 Training accuracy = 0.7703583 Loss = 0.46614546\n",
            "73 Training accuracy = 0.77198696 Loss = 0.46581802\n",
            "74 Training accuracy = 0.77198696 Loss = 0.46549132\n",
            "75 Training accuracy = 0.7703583 Loss = 0.4651639\n",
            "76 Training accuracy = 0.77198696 Loss = 0.46483514\n",
            "77 Training accuracy = 0.77198696 Loss = 0.46450508\n",
            "78 Training accuracy = 0.7703583 Loss = 0.46417677\n",
            "79 Training accuracy = 0.7703583 Loss = 0.46385664\n",
            "80 Training accuracy = 0.7703583 Loss = 0.4635527\n",
            "81 Training accuracy = 0.7703583 Loss = 0.46327022\n",
            "82 Training accuracy = 0.7703583 Loss = 0.4630107\n",
            "83 Training accuracy = 0.77198696 Loss = 0.46277207\n",
            "84 Training accuracy = 0.7703583 Loss = 0.46255037\n",
            "85 Training accuracy = 0.7703583 Loss = 0.46233782\n",
            "86 Training accuracy = 0.7703583 Loss = 0.46212417\n",
            "87 Training accuracy = 0.7687296 Loss = 0.46189782\n",
            "88 Training accuracy = 0.7703583 Loss = 0.46164876\n",
            "89 Training accuracy = 0.7687296 Loss = 0.4613701\n",
            "90 Training accuracy = 0.7687296 Loss = 0.46105808\n",
            "91 Training accuracy = 0.7687296 Loss = 0.4607112\n",
            "92 Training accuracy = 0.7654723 Loss = 0.46033132\n",
            "93 Training accuracy = 0.7654723 Loss = 0.45992225\n",
            "94 Training accuracy = 0.7654723 Loss = 0.45948896\n",
            "95 Training accuracy = 0.7654723 Loss = 0.45903537\n",
            "96 Training accuracy = 0.7687296 Loss = 0.45856392\n",
            "97 Training accuracy = 0.7687296 Loss = 0.45807594\n",
            "98 Training accuracy = 0.7703583 Loss = 0.45757157\n",
            "99 Training accuracy = 0.7703583 Loss = 0.45704958\n",
            "100 Training accuracy = 0.77198696 Loss = 0.45650828\n",
            "101 Training accuracy = 0.7703583 Loss = 0.45594633\n",
            "102 Training accuracy = 0.77198696 Loss = 0.45536378\n",
            "103 Training accuracy = 0.7752443 Loss = 0.45476148\n",
            "104 Training accuracy = 0.7752443 Loss = 0.45414123\n",
            "105 Training accuracy = 0.77850163 Loss = 0.45350608\n",
            "106 Training accuracy = 0.78175896 Loss = 0.45285964\n",
            "107 Training accuracy = 0.7833876 Loss = 0.4522055\n",
            "108 Training accuracy = 0.7850163 Loss = 0.45154694\n",
            "109 Training accuracy = 0.7850163 Loss = 0.45088723\n",
            "110 Training accuracy = 0.78664494 Loss = 0.45022866\n",
            "111 Training accuracy = 0.78664494 Loss = 0.44957396\n",
            "112 Training accuracy = 0.78664494 Loss = 0.4489248\n",
            "113 Training accuracy = 0.78664494 Loss = 0.44828328\n",
            "114 Training accuracy = 0.78827363 Loss = 0.44765162\n",
            "115 Training accuracy = 0.78827363 Loss = 0.44703168\n",
            "116 Training accuracy = 0.78664494 Loss = 0.44642514\n",
            "117 Training accuracy = 0.7833876 Loss = 0.4458333\n",
            "118 Training accuracy = 0.7850163 Loss = 0.44525692\n",
            "119 Training accuracy = 0.7833876 Loss = 0.4446959\n",
            "120 Training accuracy = 0.78013027 Loss = 0.4441497\n",
            "121 Training accuracy = 0.78013027 Loss = 0.44361714\n",
            "122 Training accuracy = 0.77850163 Loss = 0.44309708\n",
            "123 Training accuracy = 0.77850163 Loss = 0.44258857\n",
            "124 Training accuracy = 0.78013027 Loss = 0.44209057\n",
            "125 Training accuracy = 0.7833876 Loss = 0.44160208\n",
            "126 Training accuracy = 0.78013027 Loss = 0.4411226\n",
            "127 Training accuracy = 0.78013027 Loss = 0.44065085\n",
            "128 Training accuracy = 0.7833876 Loss = 0.44018608\n",
            "129 Training accuracy = 0.78175896 Loss = 0.4397269\n",
            "130 Training accuracy = 0.7833876 Loss = 0.4392724\n",
            "131 Training accuracy = 0.7833876 Loss = 0.43882185\n",
            "132 Training accuracy = 0.78175896 Loss = 0.43837512\n",
            "133 Training accuracy = 0.7833876 Loss = 0.43793222\n",
            "134 Training accuracy = 0.7833876 Loss = 0.43749395\n",
            "135 Training accuracy = 0.7850163 Loss = 0.4370607\n",
            "136 Training accuracy = 0.7850163 Loss = 0.43663338\n",
            "137 Training accuracy = 0.7850163 Loss = 0.4362123\n",
            "138 Training accuracy = 0.7850163 Loss = 0.43579772\n",
            "139 Training accuracy = 0.7850163 Loss = 0.43538967\n",
            "140 Training accuracy = 0.78827363 Loss = 0.4349881\n",
            "141 Training accuracy = 0.78827363 Loss = 0.43459272\n",
            "142 Training accuracy = 0.78827363 Loss = 0.43420327\n",
            "143 Training accuracy = 0.78990227 Loss = 0.43381912\n",
            "144 Training accuracy = 0.78990227 Loss = 0.43343922\n",
            "145 Training accuracy = 0.78990227 Loss = 0.4330629\n",
            "146 Training accuracy = 0.78827363 Loss = 0.43268925\n",
            "147 Training accuracy = 0.78827363 Loss = 0.43231717\n",
            "148 Training accuracy = 0.78827363 Loss = 0.43194574\n",
            "149 Training accuracy = 0.78827363 Loss = 0.43157443\n",
            "150 Training accuracy = 0.78664494 Loss = 0.43120274\n",
            "151 Training accuracy = 0.78664494 Loss = 0.43083018\n",
            "152 Training accuracy = 0.7850163 Loss = 0.4304567\n",
            "153 Training accuracy = 0.7850163 Loss = 0.430082\n",
            "154 Training accuracy = 0.7833876 Loss = 0.42970598\n",
            "155 Training accuracy = 0.7833876 Loss = 0.42932865\n",
            "156 Training accuracy = 0.7850163 Loss = 0.4289498\n",
            "157 Training accuracy = 0.7850163 Loss = 0.42856944\n",
            "158 Training accuracy = 0.7850163 Loss = 0.42818722\n",
            "159 Training accuracy = 0.78664494 Loss = 0.42780277\n",
            "160 Training accuracy = 0.78664494 Loss = 0.42741567\n",
            "161 Training accuracy = 0.7850163 Loss = 0.4270251\n",
            "162 Training accuracy = 0.7833876 Loss = 0.42663062\n",
            "163 Training accuracy = 0.7833876 Loss = 0.4262315\n",
            "164 Training accuracy = 0.7833876 Loss = 0.42582703\n",
            "165 Training accuracy = 0.7833876 Loss = 0.42541698\n",
            "166 Training accuracy = 0.78175896 Loss = 0.42500097\n",
            "167 Training accuracy = 0.78175896 Loss = 0.42457893\n",
            "168 Training accuracy = 0.78175896 Loss = 0.42415133\n",
            "169 Training accuracy = 0.7833876 Loss = 0.42371878\n",
            "170 Training accuracy = 0.7833876 Loss = 0.42328188\n",
            "171 Training accuracy = 0.78664494 Loss = 0.42284152\n",
            "172 Training accuracy = 0.78664494 Loss = 0.42239842\n",
            "173 Training accuracy = 0.78664494 Loss = 0.42195323\n",
            "174 Training accuracy = 0.78990227 Loss = 0.4215064\n",
            "175 Training accuracy = 0.7931596 Loss = 0.4210581\n",
            "176 Training accuracy = 0.7931596 Loss = 0.42060846\n",
            "177 Training accuracy = 0.7947883 Loss = 0.42015725\n",
            "178 Training accuracy = 0.7947883 Loss = 0.41970447\n",
            "179 Training accuracy = 0.7931596 Loss = 0.4192495\n",
            "180 Training accuracy = 0.7931596 Loss = 0.41879183\n",
            "181 Training accuracy = 0.7931596 Loss = 0.41833064\n",
            "182 Training accuracy = 0.7931596 Loss = 0.41786528\n",
            "183 Training accuracy = 0.7931596 Loss = 0.41739482\n",
            "184 Training accuracy = 0.7947883 Loss = 0.4169183\n",
            "185 Training accuracy = 0.79641694 Loss = 0.41643494\n",
            "186 Training accuracy = 0.79641694 Loss = 0.4159439\n",
            "187 Training accuracy = 0.79641694 Loss = 0.4154447\n",
            "188 Training accuracy = 0.7980456 Loss = 0.4149368\n",
            "189 Training accuracy = 0.7980456 Loss = 0.41441995\n",
            "190 Training accuracy = 0.7980456 Loss = 0.41389424\n",
            "191 Training accuracy = 0.7980456 Loss = 0.41335988\n",
            "192 Training accuracy = 0.7980456 Loss = 0.4128173\n",
            "193 Training accuracy = 0.7980456 Loss = 0.41226718\n",
            "194 Training accuracy = 0.7980456 Loss = 0.41171038\n",
            "195 Training accuracy = 0.7980456 Loss = 0.41114768\n",
            "196 Training accuracy = 0.7980456 Loss = 0.41058013\n",
            "197 Training accuracy = 0.7980456 Loss = 0.4100087\n",
            "198 Training accuracy = 0.7980456 Loss = 0.40943423\n",
            "199 Training accuracy = 0.7980456 Loss = 0.40885752\n",
            "200 Training accuracy = 0.7980456 Loss = 0.40827918\n",
            "201 Training accuracy = 0.79641694 Loss = 0.40769982\n",
            "202 Training accuracy = 0.7996743 Loss = 0.40712\n",
            "203 Training accuracy = 0.8013029 Loss = 0.40653986\n",
            "204 Training accuracy = 0.7996743 Loss = 0.4059597\n",
            "205 Training accuracy = 0.7996743 Loss = 0.40537974\n",
            "206 Training accuracy = 0.7996743 Loss = 0.40480018\n",
            "207 Training accuracy = 0.7996743 Loss = 0.40422112\n",
            "208 Training accuracy = 0.8013029 Loss = 0.4036428\n",
            "209 Training accuracy = 0.8013029 Loss = 0.4030655\n",
            "210 Training accuracy = 0.8029316 Loss = 0.40248954\n",
            "211 Training accuracy = 0.8029316 Loss = 0.40191507\n",
            "212 Training accuracy = 0.8029316 Loss = 0.4013426\n",
            "213 Training accuracy = 0.80456024 Loss = 0.40077248\n",
            "214 Training accuracy = 0.8078176 Loss = 0.400205\n",
            "215 Training accuracy = 0.8078176 Loss = 0.3996406\n",
            "216 Training accuracy = 0.8078176 Loss = 0.39907956\n",
            "217 Training accuracy = 0.8094463 Loss = 0.39852223\n",
            "218 Training accuracy = 0.8094463 Loss = 0.39796883\n",
            "219 Training accuracy = 0.8078176 Loss = 0.3974195\n",
            "220 Training accuracy = 0.8094463 Loss = 0.3968742\n",
            "221 Training accuracy = 0.8094463 Loss = 0.39633292\n",
            "222 Training accuracy = 0.8110749 Loss = 0.39579555\n",
            "223 Training accuracy = 0.8094463 Loss = 0.3952618\n",
            "224 Training accuracy = 0.8094463 Loss = 0.39473143\n",
            "225 Training accuracy = 0.8094463 Loss = 0.39420417\n",
            "226 Training accuracy = 0.8094463 Loss = 0.39367983\n",
            "227 Training accuracy = 0.8094463 Loss = 0.39315817\n",
            "228 Training accuracy = 0.8094463 Loss = 0.39263898\n",
            "229 Training accuracy = 0.8094463 Loss = 0.39212212\n",
            "230 Training accuracy = 0.8110749 Loss = 0.3916074\n",
            "231 Training accuracy = 0.8127036 Loss = 0.39109468\n",
            "232 Training accuracy = 0.8127036 Loss = 0.39058384\n",
            "233 Training accuracy = 0.8110749 Loss = 0.3900747\n",
            "234 Training accuracy = 0.8127036 Loss = 0.38956708\n",
            "235 Training accuracy = 0.8127036 Loss = 0.38906077\n",
            "236 Training accuracy = 0.8110749 Loss = 0.3885555\n",
            "237 Training accuracy = 0.8110749 Loss = 0.388051\n",
            "238 Training accuracy = 0.8127036 Loss = 0.38754708\n",
            "239 Training accuracy = 0.8110749 Loss = 0.3870433\n",
            "240 Training accuracy = 0.8110749 Loss = 0.3865395\n",
            "241 Training accuracy = 0.8110749 Loss = 0.38603517\n",
            "242 Training accuracy = 0.8110749 Loss = 0.38553014\n",
            "243 Training accuracy = 0.8110749 Loss = 0.38502383\n",
            "244 Training accuracy = 0.8094463 Loss = 0.3845162\n",
            "245 Training accuracy = 0.8094463 Loss = 0.38400695\n",
            "246 Training accuracy = 0.8094463 Loss = 0.38349584\n",
            "247 Training accuracy = 0.8094463 Loss = 0.38298276\n",
            "248 Training accuracy = 0.8078176 Loss = 0.38246766\n",
            "249 Training accuracy = 0.8078176 Loss = 0.38195062\n",
            "250 Training accuracy = 0.8078176 Loss = 0.38143155\n",
            "251 Training accuracy = 0.8110749 Loss = 0.3809106\n",
            "252 Training accuracy = 0.8127036 Loss = 0.38038793\n",
            "253 Training accuracy = 0.81433225 Loss = 0.37986347\n",
            "254 Training accuracy = 0.8159609 Loss = 0.3793374\n",
            "255 Training accuracy = 0.8175896 Loss = 0.3788097\n",
            "256 Training accuracy = 0.8159609 Loss = 0.37828025\n",
            "257 Training accuracy = 0.81433225 Loss = 0.37774906\n",
            "258 Training accuracy = 0.81433225 Loss = 0.3772158\n",
            "259 Training accuracy = 0.81433225 Loss = 0.3766803\n",
            "260 Training accuracy = 0.8175896 Loss = 0.37614217\n",
            "261 Training accuracy = 0.82247555 Loss = 0.37560108\n",
            "262 Training accuracy = 0.8273616 Loss = 0.37505668\n",
            "263 Training accuracy = 0.8273616 Loss = 0.3745085\n",
            "264 Training accuracy = 0.8273616 Loss = 0.37395635\n",
            "265 Training accuracy = 0.8289902 Loss = 0.37339985\n",
            "266 Training accuracy = 0.8273616 Loss = 0.37283888\n",
            "267 Training accuracy = 0.8273616 Loss = 0.37227353\n",
            "268 Training accuracy = 0.8273616 Loss = 0.3717038\n",
            "269 Training accuracy = 0.8273616 Loss = 0.37112984\n",
            "270 Training accuracy = 0.8273616 Loss = 0.37055203\n",
            "271 Training accuracy = 0.8273616 Loss = 0.36997068\n",
            "272 Training accuracy = 0.8273616 Loss = 0.36938623\n",
            "273 Training accuracy = 0.8273616 Loss = 0.36879903\n",
            "274 Training accuracy = 0.8289902 Loss = 0.36820912\n",
            "275 Training accuracy = 0.8306189 Loss = 0.36761677\n",
            "276 Training accuracy = 0.83224756 Loss = 0.367022\n",
            "277 Training accuracy = 0.83224756 Loss = 0.36642456\n",
            "278 Training accuracy = 0.83224756 Loss = 0.3658242\n",
            "279 Training accuracy = 0.83224756 Loss = 0.3652205\n",
            "280 Training accuracy = 0.83224756 Loss = 0.36461312\n",
            "281 Training accuracy = 0.8355049 Loss = 0.36400154\n",
            "282 Training accuracy = 0.8338762 Loss = 0.36338526\n",
            "283 Training accuracy = 0.8371335 Loss = 0.3627637\n",
            "284 Training accuracy = 0.8371335 Loss = 0.3621363\n",
            "285 Training accuracy = 0.8338762 Loss = 0.36150235\n",
            "286 Training accuracy = 0.83224756 Loss = 0.3608613\n",
            "287 Training accuracy = 0.83224756 Loss = 0.3602122\n",
            "288 Training accuracy = 0.83224756 Loss = 0.35955438\n",
            "289 Training accuracy = 0.83224756 Loss = 0.35888702\n",
            "290 Training accuracy = 0.83224756 Loss = 0.35820934\n",
            "291 Training accuracy = 0.83224756 Loss = 0.35752055\n",
            "292 Training accuracy = 0.83224756 Loss = 0.35681993\n",
            "293 Training accuracy = 0.83224756 Loss = 0.35610682\n",
            "294 Training accuracy = 0.8306189 Loss = 0.35538056\n",
            "295 Training accuracy = 0.8306189 Loss = 0.35464066\n",
            "296 Training accuracy = 0.8306189 Loss = 0.35388654\n",
            "297 Training accuracy = 0.8306189 Loss = 0.35311773\n",
            "298 Training accuracy = 0.8306189 Loss = 0.35233375\n",
            "299 Training accuracy = 0.83224756 Loss = 0.35153416\n",
            "300 Training accuracy = 0.83224756 Loss = 0.35071856\n",
            "301 Training accuracy = 0.83224756 Loss = 0.34988636\n",
            "302 Training accuracy = 0.8306189 Loss = 0.34903708\n",
            "303 Training accuracy = 0.8338762 Loss = 0.34817004\n",
            "304 Training accuracy = 0.8355049 Loss = 0.34728435\n",
            "305 Training accuracy = 0.8371335 Loss = 0.34637898\n",
            "306 Training accuracy = 0.84039086 Loss = 0.34545264\n",
            "307 Training accuracy = 0.84201956 Loss = 0.34450367\n",
            "308 Training accuracy = 0.8436482 Loss = 0.34353027\n",
            "309 Training accuracy = 0.8436482 Loss = 0.34253016\n",
            "310 Training accuracy = 0.8436482 Loss = 0.34150103\n",
            "311 Training accuracy = 0.8469055 Loss = 0.34044024\n",
            "312 Training accuracy = 0.8485342 Loss = 0.33934534\n",
            "313 Training accuracy = 0.8485342 Loss = 0.3382143\n",
            "314 Training accuracy = 0.8469055 Loss = 0.33704633\n",
            "315 Training accuracy = 0.8452769 Loss = 0.33584207\n",
            "316 Training accuracy = 0.8469055 Loss = 0.33460394\n",
            "317 Training accuracy = 0.8469055 Loss = 0.3333355\n",
            "318 Training accuracy = 0.8485342 Loss = 0.3320404\n",
            "319 Training accuracy = 0.8485342 Loss = 0.33072123\n",
            "320 Training accuracy = 0.8485342 Loss = 0.32938015\n",
            "321 Training accuracy = 0.8469055 Loss = 0.32801884\n",
            "322 Training accuracy = 0.8469055 Loss = 0.32663944\n",
            "323 Training accuracy = 0.85016286 Loss = 0.32524487\n",
            "324 Training accuracy = 0.85016286 Loss = 0.32383886\n",
            "325 Training accuracy = 0.85179156 Loss = 0.32242584\n",
            "326 Training accuracy = 0.85179156 Loss = 0.32100973\n",
            "327 Training accuracy = 0.8534202 Loss = 0.3195941\n",
            "328 Training accuracy = 0.8485342 Loss = 0.3181813\n",
            "329 Training accuracy = 0.8485342 Loss = 0.31677336\n",
            "330 Training accuracy = 0.8534202 Loss = 0.31537133\n",
            "331 Training accuracy = 0.8485342 Loss = 0.31397623\n",
            "332 Training accuracy = 0.8485342 Loss = 0.31258893\n",
            "333 Training accuracy = 0.8485342 Loss = 0.31121013\n",
            "334 Training accuracy = 0.8485342 Loss = 0.30984017\n",
            "335 Training accuracy = 0.8469055 Loss = 0.30847904\n",
            "336 Training accuracy = 0.8469055 Loss = 0.3071257\n",
            "337 Training accuracy = 0.85016286 Loss = 0.3057785\n",
            "338 Training accuracy = 0.85016286 Loss = 0.30443507\n",
            "339 Training accuracy = 0.85179156 Loss = 0.30309328\n",
            "340 Training accuracy = 0.85179156 Loss = 0.3017514\n",
            "341 Training accuracy = 0.85504884 Loss = 0.3004093\n",
            "342 Training accuracy = 0.85504884 Loss = 0.29906812\n",
            "343 Training accuracy = 0.85179156 Loss = 0.2977305\n",
            "344 Training accuracy = 0.85016286 Loss = 0.29639998\n",
            "345 Training accuracy = 0.85179156 Loss = 0.29508054\n",
            "346 Training accuracy = 0.85667753 Loss = 0.29377574\n",
            "347 Training accuracy = 0.85504884 Loss = 0.29248857\n",
            "348 Training accuracy = 0.85667753 Loss = 0.29122123\n",
            "349 Training accuracy = 0.85504884 Loss = 0.28997493\n",
            "350 Training accuracy = 0.85993487 Loss = 0.28875044\n",
            "351 Training accuracy = 0.85993487 Loss = 0.28754807\n",
            "352 Training accuracy = 0.85993487 Loss = 0.28636754\n",
            "353 Training accuracy = 0.86482084 Loss = 0.2852084\n",
            "354 Training accuracy = 0.86644953 Loss = 0.28406987\n",
            "355 Training accuracy = 0.8680782 Loss = 0.28295052\n",
            "356 Training accuracy = 0.86644953 Loss = 0.28184906\n",
            "357 Training accuracy = 0.86644953 Loss = 0.28076404\n",
            "358 Training accuracy = 0.86644953 Loss = 0.27969432\n",
            "359 Training accuracy = 0.86482084 Loss = 0.27863878\n",
            "360 Training accuracy = 0.8631922 Loss = 0.27759683\n",
            "361 Training accuracy = 0.8631922 Loss = 0.27656817\n",
            "362 Training accuracy = 0.8631922 Loss = 0.27555272\n",
            "363 Training accuracy = 0.8631922 Loss = 0.27455133\n",
            "364 Training accuracy = 0.86482084 Loss = 0.27356946\n",
            "365 Training accuracy = 0.86644953 Loss = 0.27263278\n",
            "366 Training accuracy = 0.8713355 Loss = 0.27183726\n",
            "367 Training accuracy = 0.87459284 Loss = 0.27139163\n",
            "368 Training accuracy = 0.86970687 Loss = 0.2706839\n",
            "369 Training accuracy = 0.87296414 Loss = 0.26911917\n",
            "370 Training accuracy = 0.87296414 Loss = 0.2683236\n",
            "371 Training accuracy = 0.87296414 Loss = 0.2678781\n",
            "372 Training accuracy = 0.8762215 Loss = 0.26654726\n",
            "373 Training accuracy = 0.8762215 Loss = 0.2659436\n",
            "374 Training accuracy = 0.87459284 Loss = 0.2652238\n",
            "375 Training accuracy = 0.8811075 Loss = 0.264117\n",
            "376 Training accuracy = 0.8762215 Loss = 0.26364142\n",
            "377 Training accuracy = 0.8876222 Loss = 0.26264977\n",
            "378 Training accuracy = 0.8859935 Loss = 0.2619031\n",
            "379 Training accuracy = 0.8778502 Loss = 0.26124597\n",
            "380 Training accuracy = 0.88436484 Loss = 0.2602902\n",
            "381 Training accuracy = 0.8892508 Loss = 0.25972572\n",
            "382 Training accuracy = 0.88273615 Loss = 0.25885192\n",
            "383 Training accuracy = 0.8859935 Loss = 0.25814632\n",
            "384 Training accuracy = 0.89087945 Loss = 0.25746465\n",
            "385 Training accuracy = 0.8876222 Loss = 0.25663644\n",
            "386 Training accuracy = 0.88436484 Loss = 0.25603178\n",
            "387 Training accuracy = 0.8892508 Loss = 0.2552333\n",
            "388 Training accuracy = 0.89087945 Loss = 0.25456777\n",
            "389 Training accuracy = 0.8876222 Loss = 0.253882\n",
            "390 Training accuracy = 0.89250815 Loss = 0.25313902\n",
            "391 Training accuracy = 0.89087945 Loss = 0.25251922\n",
            "392 Training accuracy = 0.89087945 Loss = 0.25178042\n",
            "393 Training accuracy = 0.8892508 Loss = 0.2511363\n",
            "394 Training accuracy = 0.89087945 Loss = 0.25046405\n",
            "395 Training accuracy = 0.89250815 Loss = 0.24977052\n",
            "396 Training accuracy = 0.8876222 Loss = 0.24914414\n",
            "397 Training accuracy = 0.89250815 Loss = 0.248447\n",
            "398 Training accuracy = 0.89250815 Loss = 0.2478094\n",
            "399 Training accuracy = 0.89250815 Loss = 0.24715167\n",
            "400 Training accuracy = 0.89250815 Loss = 0.24648319\n",
            "401 Training accuracy = 0.8941368 Loss = 0.2458553\n",
            "402 Training accuracy = 0.89250815 Loss = 0.24518603\n",
            "403 Training accuracy = 0.89250815 Loss = 0.24455078\n",
            "404 Training accuracy = 0.8941368 Loss = 0.24390894\n",
            "405 Training accuracy = 0.8941368 Loss = 0.24325573\n",
            "406 Training accuracy = 0.89250815 Loss = 0.24262986\n",
            "407 Training accuracy = 0.8941368 Loss = 0.24197923\n",
            "408 Training accuracy = 0.8957655 Loss = 0.24134247\n",
            "409 Training accuracy = 0.8957655 Loss = 0.24070445\n",
            "410 Training accuracy = 0.8973941 Loss = 0.2400497\n",
            "411 Training accuracy = 0.8973941 Loss = 0.23940602\n",
            "412 Training accuracy = 0.8990228 Loss = 0.23874196\n",
            "413 Training accuracy = 0.8990228 Loss = 0.23807128\n",
            "414 Training accuracy = 0.90065145 Loss = 0.23739688\n",
            "415 Training accuracy = 0.90065145 Loss = 0.23670134\n",
            "416 Training accuracy = 0.90065145 Loss = 0.23600538\n",
            "417 Training accuracy = 0.90065145 Loss = 0.23529823\n",
            "418 Training accuracy = 0.90065145 Loss = 0.2345805\n",
            "419 Training accuracy = 0.8990228 Loss = 0.23386413\n",
            "420 Training accuracy = 0.8990228 Loss = 0.23313752\n",
            "421 Training accuracy = 0.8990228 Loss = 0.23240955\n",
            "422 Training accuracy = 0.8990228 Loss = 0.23168103\n",
            "423 Training accuracy = 0.8990228 Loss = 0.2309458\n",
            "424 Training accuracy = 0.8990228 Loss = 0.23021238\n",
            "425 Training accuracy = 0.90228015 Loss = 0.22947659\n",
            "426 Training accuracy = 0.90228015 Loss = 0.2287387\n",
            "427 Training accuracy = 0.90065145 Loss = 0.22800505\n",
            "428 Training accuracy = 0.90228015 Loss = 0.22727114\n",
            "429 Training accuracy = 0.9039088 Loss = 0.22654018\n",
            "430 Training accuracy = 0.9039088 Loss = 0.225814\n",
            "431 Training accuracy = 0.9039088 Loss = 0.22508895\n",
            "432 Training accuracy = 0.9039088 Loss = 0.22436848\n",
            "433 Training accuracy = 0.90228015 Loss = 0.22365247\n",
            "434 Training accuracy = 0.90228015 Loss = 0.22293864\n",
            "435 Training accuracy = 0.90228015 Loss = 0.22222963\n",
            "436 Training accuracy = 0.90228015 Loss = 0.22152355\n",
            "437 Training accuracy = 0.90228015 Loss = 0.22081925\n",
            "438 Training accuracy = 0.9039088 Loss = 0.22011815\n",
            "439 Training accuracy = 0.90228015 Loss = 0.2194186\n",
            "440 Training accuracy = 0.9039088 Loss = 0.21872033\n",
            "441 Training accuracy = 0.9039088 Loss = 0.21802449\n",
            "442 Training accuracy = 0.9039088 Loss = 0.21733004\n",
            "443 Training accuracy = 0.9039088 Loss = 0.21663715\n",
            "444 Training accuracy = 0.90228015 Loss = 0.21594673\n",
            "445 Training accuracy = 0.90228015 Loss = 0.215258\n",
            "446 Training accuracy = 0.90228015 Loss = 0.2145713\n",
            "447 Training accuracy = 0.90228015 Loss = 0.2138872\n",
            "448 Training accuracy = 0.90228015 Loss = 0.21320504\n",
            "449 Training accuracy = 0.90228015 Loss = 0.21252497\n",
            "450 Training accuracy = 0.90228015 Loss = 0.21184714\n",
            "451 Training accuracy = 0.90065145 Loss = 0.21117106\n",
            "452 Training accuracy = 0.90065145 Loss = 0.21049644\n",
            "453 Training accuracy = 0.90065145 Loss = 0.20982327\n",
            "454 Training accuracy = 0.90228015 Loss = 0.20915078\n",
            "455 Training accuracy = 0.90065145 Loss = 0.2084785\n",
            "456 Training accuracy = 0.9039088 Loss = 0.20780602\n",
            "457 Training accuracy = 0.9055375 Loss = 0.20713244\n",
            "458 Training accuracy = 0.9055375 Loss = 0.20645691\n",
            "459 Training accuracy = 0.9055375 Loss = 0.20577872\n",
            "460 Training accuracy = 0.9055375 Loss = 0.20509678\n",
            "461 Training accuracy = 0.9136808 Loss = 0.20441018\n",
            "462 Training accuracy = 0.9153094 Loss = 0.20371826\n",
            "463 Training accuracy = 0.9169381 Loss = 0.20302013\n",
            "464 Training accuracy = 0.9153094 Loss = 0.20231509\n",
            "465 Training accuracy = 0.91856676 Loss = 0.20160244\n",
            "466 Training accuracy = 0.91856676 Loss = 0.20088144\n",
            "467 Training accuracy = 0.91856676 Loss = 0.20015101\n",
            "468 Training accuracy = 0.91856676 Loss = 0.19941013\n",
            "469 Training accuracy = 0.92019546 Loss = 0.19865757\n",
            "470 Training accuracy = 0.92019546 Loss = 0.19789211\n",
            "471 Training accuracy = 0.9218241 Loss = 0.19711225\n",
            "472 Training accuracy = 0.9218241 Loss = 0.1963169\n",
            "473 Training accuracy = 0.92019546 Loss = 0.19550484\n",
            "474 Training accuracy = 0.92019546 Loss = 0.19467512\n",
            "475 Training accuracy = 0.91856676 Loss = 0.19382723\n",
            "476 Training accuracy = 0.91856676 Loss = 0.19296072\n",
            "477 Training accuracy = 0.91856676 Loss = 0.19207577\n",
            "478 Training accuracy = 0.92019546 Loss = 0.19117264\n",
            "479 Training accuracy = 0.92019546 Loss = 0.19025213\n",
            "480 Training accuracy = 0.92019546 Loss = 0.1893153\n",
            "481 Training accuracy = 0.92019546 Loss = 0.18836348\n",
            "482 Training accuracy = 0.92019546 Loss = 0.18739812\n",
            "483 Training accuracy = 0.9218241 Loss = 0.18642114\n",
            "484 Training accuracy = 0.92019546 Loss = 0.1854345\n",
            "485 Training accuracy = 0.9218241 Loss = 0.18444045\n",
            "486 Training accuracy = 0.9218241 Loss = 0.1834415\n",
            "487 Training accuracy = 0.9218241 Loss = 0.18244036\n",
            "488 Training accuracy = 0.9218241 Loss = 0.1814399\n",
            "489 Training accuracy = 0.9234528 Loss = 0.180443\n",
            "490 Training accuracy = 0.9234528 Loss = 0.1794526\n",
            "491 Training accuracy = 0.9218241 Loss = 0.17847142\n",
            "492 Training accuracy = 0.9218241 Loss = 0.17750177\n",
            "493 Training accuracy = 0.92508143 Loss = 0.17654535\n",
            "494 Training accuracy = 0.92508143 Loss = 0.17560303\n",
            "495 Training accuracy = 0.92508143 Loss = 0.17467505\n",
            "496 Training accuracy = 0.92671007 Loss = 0.17376076\n",
            "497 Training accuracy = 0.92671007 Loss = 0.17285885\n",
            "498 Training accuracy = 0.92833877 Loss = 0.1719681\n",
            "499 Training accuracy = 0.92833877 Loss = 0.1710872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91MNdOv9ROGp",
        "colab_type": "code",
        "outputId": "934f45d0-bd9d-426d-c1a9-a59a6c632099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Step 6: Evaluation\n",
        "test_data = {X: X_test, y: y_test1}\n",
        "print(\"Testing Accuracy = \", sess.run(accuracy, feed_dict = test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy =  0.7012987\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C5QXF1jRUVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}